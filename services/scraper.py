import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List
from models.sauna import SaunaBase
from urllib.parse import urljoin
import logging
import time
import os
from collections import defaultdict
from sqlalchemy.orm import Session
from models.database import ScrapingState
from sqlalchemy import select

logger = logging.getLogger(__name__)

class SaunaScraper:
    def __init__(self):
        self.base_url = "https://sauna-ikitai.com"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }        
        # ãƒšãƒ¼ã‚¸é–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
        self.wait_time = 3
        self.DEFAULT_START_PAGE = 1

    def _get_page_content(self, url: str) -> BeautifulSoup:
        """æŒ‡å®šURLã®ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            print("=== ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰:", response.status_code)
            print("=== ã‚¿ã‚¤ãƒˆãƒ«:", soup.title.text if soup.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—")
            print("=== HTMLå†’é ­ ===")
            print(soup.prettify()[:1000])  # é•·ã™ãé˜²æ­¢

            return soup

        except requests.RequestException as e:
            logger.error(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    def scrape_sauna_reviews(self, target_url: str = None) -> List[SaunaBase]:
        """
        ã€Œç©´å ´ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã‚‰ã€ã‚µã‚¦ãƒŠæƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

        Args:
            target_url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URLï¼ˆNoneã®å ´åˆã¯self.base_urlã‚’ä½¿ç”¨ï¼‰

        Returns:
            List[SaunaBase]: ã‚µã‚¦ãƒŠæƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        saunas = []
        url_to_scrape = target_url or self.base_url
        soup = self._get_page_content(url_to_scrape)
        print("ğŸ”— ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚‹URL:", url_to_scrape)
        print("ğŸ“„ HTMLå…ˆé ­1000æ–‡å­—:\n", soup.prettify()[:1000])
        try:
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸€è¦§ã®è¦ç´ ã‚’å–å¾—
            review_items = soup.select(".p-postCard")
            print(f"review_items ä»¶æ•°: {len(review_items)}")  
            for item in review_items:
                try:
                    # ã‚µã‚¦ãƒŠåã¨URLã‚’å–å¾—
                    name_link_element = item.select_one(".p-postCard_facility a")
                    
                    if name_link_element:
                        name = name_link_element.text.strip()
                        url = name_link_element.get("href")
                        
                        # URLãŒç›¸å¯¾ãƒ‘ã‚¹ã§ãªãå®Œå…¨URLã«ãªã£ã¦ã„ã‚‹ãªã‚‰ãã®ã¾ã¾ä½¿ã†
                        full_url = url if url.startswith("http") else urljoin(self.base_url, url)

                        # å„ãƒ¬ãƒ“ãƒ¥ãƒ¼ = 1ã‚«ã‚¦ãƒ³ãƒˆã¨ã—ã¦æ‰±ã†
                        review_count = 1

                        sauna = SaunaBase(
                            name=name,
                            url=full_url,
                            review_count=review_count,
                            last_updated=datetime.now()
                        )
                        saunas.append(sauna)

                except Exception as e:
                    logger.error(f"ã‚µã‚¦ãƒŠæƒ…å ±ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    continue

        except Exception as e:
            logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            raise

        return saunas
   
    def aggregate_saunas(self, sauna_list: List[SaunaBase]) -> List[SaunaBase]:
        """åŒä¸€URLã®ã‚µã‚¦ãƒŠã‚’1ã¤ã«ã¾ã¨ã‚ã¦ã€review_countã‚’åˆç®—ã™ã‚‹"""
        aggregated = {}

        for sauna in sauna_list:
            key = str(sauna.url)
            if key in aggregated:
                aggregated[key].review_count += sauna.review_count
            else:
                # copyã—ãªã„ã¨å…ƒã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å†åˆ©ç”¨ã™ã‚‹ã“ã¨ã«ãªã‚‹ã®ã§ deepcopyã§ã‚‚å¯
                aggregated[key] = SaunaBase(
                    name=sauna.name,
                    url=sauna.url,
                    review_count=sauna.review_count,
                    last_updated=sauna.last_updated
                )

        return list(aggregated.values())

    def get_review_count(self, sauna_url: str) -> int:
        """
        å€‹åˆ¥ã®ã‚µã‚¦ãƒŠãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ã‚’å–å¾—ï¼ˆå¿…è¦ãªå ´åˆã«å®Ÿè£…ï¼‰
        
        Args:
            sauna_url: ã‚µã‚¦ãƒŠã®è©³ç´°ãƒšãƒ¼ã‚¸URL
            
        Returns:
            int: ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
        """
        try:
            soup = self._get_page_content(sauna_url)
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ã‚’å«ã‚€è¦ç´ ã‚’å–å¾—ï¼ˆã‚»ãƒ¬ã‚¯ã‚¿ã¯å®Ÿéš›ã®ã‚‚ã®ã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰
            review_element = soup.select_one(".review-count")
            if review_element:
                count_text = review_element.text.strip()
                return int(''.join(filter(str.isdigit, count_text)))
            return 0
        except Exception as e:
            logger.error(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return 0 

    def generate_page_url(self, page: int) -> str:
        """ãƒšãƒ¼ã‚¸ç•ªå·ã‹ã‚‰URLã‚’ç”Ÿæˆ"""
        return f"{self.base_url}/posts?keyword=ç©´å ´&page={page}&prefecture[0]=tokyo"

    def load_last_scraped_page(self, db: Session) -> int:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€å¾Œã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸãƒšãƒ¼ã‚¸ç•ªå·ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            stmt = select(ScrapingState).where(ScrapingState.key == "last_page")
            result = db.execute(stmt).scalar_one_or_none()
            
            if result is None:
                # åˆå›å®Ÿè¡Œæ™‚ã¯æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ
                state = ScrapingState(key="last_page", value=self.DEFAULT_START_PAGE)
                db.add(state)
                db.commit()
                return self.DEFAULT_START_PAGE
                
            return result.value
            
        except Exception as e:
            logger.error(f"å‰å›ã®ãƒšãƒ¼ã‚¸æƒ…å ±ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return self.DEFAULT_START_PAGE

    def save_last_scraped_page(self, db: Session, page: int) -> None:
        """æ¬¡å›ã®ã‚¹ã‚¿ãƒ¼ãƒˆãƒšãƒ¼ã‚¸ç•ªå·ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            stmt = select(ScrapingState).where(ScrapingState.key == "last_page")
            state = db.execute(stmt).scalar_one_or_none()
            
            if state:
                state.value = page
            else:
                state = ScrapingState(key="last_page", value=page)
                db.add(state)
                
            db.commit()
            logger.info(f"æ¬¡å›ã®é–‹å§‹ãƒšãƒ¼ã‚¸ï¼ˆ{page}ï¼‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"ãƒšãƒ¼ã‚¸æƒ…å ±ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            db.rollback()

    def scrape_multiple_pages(self, start_page: int, num_pages: int = 1) -> List[SaunaBase]:
        """
        æŒ‡å®šã—ãŸãƒšãƒ¼ã‚¸æ•°åˆ†ã®ã‚µã‚¦ãƒŠæƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

        Args:
            start_page: é–‹å§‹ãƒšãƒ¼ã‚¸ç•ªå·
            num_pages: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ï¼‰

        Returns:
            List[SaunaBase]: åé›†ã—ãŸã‚µã‚¦ãƒŠæƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        all_saunas = []
        end_page = start_page + num_pages

        for page in range(start_page, end_page):
            try:
                logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™")
                url = self.generate_page_url(page)
                page_saunas = self.scrape_sauna_reviews(url)
                all_saunas.extend(page_saunas)
                
                # æœ€å¾Œã®ãƒšãƒ¼ã‚¸ä»¥å¤–ã§å¾…æ©Ÿ
                if page < end_page - 1:
                    logger.info(f"{self.wait_time}ç§’å¾…æ©Ÿã—ã¾ã™...")
                    time.sleep(self.wait_time)

            except Exception as e:
                logger.error(f"ãƒšãƒ¼ã‚¸ {page} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                break

        return all_saunas

    def run_scheduled_scraping(self, db: Session, num_pages: int = 3) -> List[SaunaBase]:
        """
        å‰å›ã®ç¶šãã‹ã‚‰æŒ‡å®šãƒšãƒ¼ã‚¸æ•°åˆ†ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
        """
        # å‰å›ã®ç¶šãã®ãƒšãƒ¼ã‚¸ã‚’DBã‹ã‚‰èª­ã¿è¾¼ã‚€
        start_page = self.load_last_scraped_page(db)
        logger.info(f"ãƒšãƒ¼ã‚¸ {start_page} ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™")

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
        saunas = self.scrape_multiple_pages(start_page, num_pages)

        # æ¬¡å›ã®é–‹å§‹ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
        next_start_page = start_page + num_pages
        self.save_last_scraped_page(db, next_start_page)

        return saunas 